{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lo último de ecuaciones trascendentales e introducción a la optimización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Método de Newton-Raphson para ecuaciones trascendentales\n",
    "\n",
    "Supongamos que $f$ es diferenciable, es decir, su primera derivada $f'(x)$ existe y es continua. Podemos aproximar $f$ en una serie de Taylor a primer orden para un punto $x+h$, $h << x$\n",
    "\n",
    "$$\n",
    "f(x+h) \\approx f(x) + f'(x)h\n",
    "$$\n",
    "\n",
    "Supongamos, que $x+h$ es solución a la ecuación trascendental ( $ f(x+h) = 0 $ )\n",
    "\n",
    "$$\n",
    "0 \\approx f(x) + f'(x)h\n",
    "$$\n",
    "\n",
    "Supongamos que conocemos $x$ y lo que estamos buscando es cuál es el valor de $h$ tal que $x+h$ es solución a la ecuación trascendental. Usando la ecuación anterior, podemos expresar $h$\n",
    "\n",
    "$$\n",
    "h = -\\frac{f(x)}{f'(x)}\n",
    "$$\n",
    "\n",
    "Esto nos permite proponer un esquema **recursivo** para construir una sucesión $x_k$ tal que converga a una solución de la ecuación.\n",
    "\n",
    "$$\n",
    "x_k = \\begin{cases}\n",
    "    x_{inicial} & \\text{ si } k=1 \\\\\n",
    "    x_{k-1} + h = x_{k-1} -\\frac{f(x_{k-1})}{f'(x_{k-1})} & \\text{ si } k>1 \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Construir dicha sucesión es el llamado **Método de Newton-Raphson**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1\n",
    "\n",
    "Define una función `miNewton(f,df,x_inicial,n)` que construya los primeros $n$ términos de la sucesión del método de Newton-Raphson para una función `f`, con derivada`df` y punto inicial `x_inicial`. Utiliza la función para resolver las ecuaciones del ejercicio 6 de la clase 07. v. ¿Converge más rápido que los otros dos métodos vistos? x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2\n",
    "\n",
    "Ahora define una función `miNewtonDif(f,x_inicial,n)` que construya los primeros $n$ términos de la sucesión del método de Newton-Raphson, aproximando la derivada $f'$ como una diferencia finita centrada, para una función `f` y punto inicial `x_inicial`. Compara su desempeño con la del ejercicio anterior y concluye cuál de las dos es mejor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3\n",
    "\n",
    "En lugar de pedir los primeros $n$ términos de la sucesión, podríamos mejor pedir que nuestra función nos generara términos de la sucesión hasta que nuestra solución a la ecuación sea **suficientemente buena**, es decir, hasta que se cumpla que $|f(x_{m})| < \\epsilon$.\n",
    "\n",
    "Define una función `miNewtonTol(f,x_inicial,epsilon)` que implemente el método de Newton-Rhapson utilizando una diferencia centrada y que genere los primeros $m$ hasta que se cumpla que `|f(x_m)| < epsilon`\n",
    "\n",
    "\n",
    "**Sugerencia:** utiliza un ciclo While"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 4\n",
    "\n",
    "El método de Newton-Rhapson se puede generalizar para encontrar raíces de funciones vectoriales $g:\\mathbb{R}^m \\to \\mathbb{R}^m$,es decir, encontrar el vector $\\mathbf{x}^*$ tal que $g(\\mathbf{x}^*) = \\mathbf{0}$. Dichas función representa un sistema de ecuaciones trascendentales. Supongamos $g(\\mathbf{x}) = (g_1(\\mathbf{x}),\\ldots,g_m(\\mathbf{x}))$. El sistema es\n",
    "\n",
    "$$\n",
    "g_1(\\mathbf{x}) = 0 \\\\\n",
    "g_2(\\mathbf{x}) = 0 \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    "g_m(\\mathbf{x}) = 0\n",
    "$$\n",
    "\n",
    "Utiliza una serie de taylor multidimensional y deduce la sucesión recursiva para el método de Newton-Rhapson multidimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción a la Optimización\n",
    "\n",
    "La optimización se puede entender como la búsqueda de valores extremales de una función $f:\\mathbb{R}^n \\to \\mathbb{R}$, es decir,  en la búsqueda de **mínimos** y **máximos** de una función. En general, lo que siempre se busca son **mínimos globales**. La optimización es un área [muy grande](https://www.google.com/search?tbm=bks&q=optimization) de las matemáticas, mucho más que las ecuaciones trascendentales, pues la función $f$ puede ser de muchas formas, no necesariamente una función real o entre espacios reales. \n",
    "\n",
    "**Nosotros solo hablaremos de la optimización de funciones reales** que, por si fuera poco, también es un área inmensamente grande.\n",
    "\n",
    "### ¿Qué es un mínimo global?\n",
    "\n",
    "Sea $f:U \\subseteq \\mathbb{R}^n \\to \\mathbb{R}$, buscamos $\\mathbf{x^*}\\in U$ tal que $\\forall \\; \\mathbf{x} \\in U \\quad f(\\mathbf{x^*}) \\leq f(\\mathbf{x})$. Decimos que $\\mathbf{x^*}$ es un **mínimo global** de $f$ en $U$\n",
    "\n",
    "### ¿Y por qué no hablamos o definimos los máximos?\n",
    "\n",
    "Sea $f:U \\subseteq \\mathbb{R}^n \\to \\mathbb{R}$, buscamos $\\mathbf{z^*}\\in U$ tal que $\\forall \\; \\mathbf{x} \\in U \\quad f(\\mathbf{z^*}) \\geq f(\\mathbf{x})$. Decimos que $\\mathbf{z^*}$ es un **máximo global** de $f$ en $U$.\n",
    "\n",
    "Encontrarle un máximo a una función $f$ es equivalente a encontrarle un mínimo a la función $g(x) = - f(x)$\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "f(\\mathbf{z^*}) \\geq f(\\mathbf{x}) \\implies -f(\\mathbf{z^*}) \\leq -f(\\mathbf{x}) \\implies g(\\mathbf{z^*}) \\leq g(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "En optimización, por convención, generalmente solo tratamos con problemas de **minimización**\n",
    "\n",
    "### ¿Es fácil encontrar mínimos globales?\n",
    "\n",
    "NO, ni siquiera en clase de cálculo se puede. Lo que si se puede, más fácilmente, es encontrar **mínimos locales**\n",
    "\n",
    "Sea $f:U \\subseteq \\mathbb{R}^n \\to \\mathbb{R}$, buscamos $\\mathbf{x^*}\\in W \\subset U$ tal que $\\forall \\; \\mathbf{x} \\in W \\quad f(\\mathbf{x^*}) \\leq f(\\mathbf{x})$. Decimos que $\\mathbf{x^*}$ es un **mínimo local** de $f$ en $U$\n",
    "\n",
    "### ¿Todas las funciones tienen mínimos locales?\n",
    "\n",
    "SI $W = \\{\\mathbf{y}\\}$, $\\mathbf{y}$ es minimo local de cualquier funcion. Si $W$ no es un solo punto, **no todas las funciones tienen mínimos locales**. Por ejemplo $f(x) = 3x +2$\n",
    "\n",
    "### ¿Es fácil saber si una función tiene mínimos locales?\n",
    "\n",
    "NO, tampoco es fácil saberlo. Si $f$ es diferenciable, y existen puntos en los que $\\nabla f = \\mathbf{0}$, **pueden existir mínimos locales, pero no es seguro**\n",
    "\n",
    "### ¿Si existen, es fácil encontrarlos?\n",
    "\n",
    "NO. Analíticamente (teóricamente), no hay una manera algorítmica de hacerlo. Los criterios que utilizan la derivada normalmente llegan a **ecuaciones trascendentales** (resolver $\\nabla f = \\mathbf{0}$) y acabamos de ver que esos problemas también son complicados. \n",
    "\n",
    "Usando algoritmos de análisis numérico, podemos acercarnos o buscar los mínimos.También puede darse que no los encontremos. **Pero es más probable encontrarlos numéricamente**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caso sencillo: funciones unimodales y unidimensionales\n",
    "\n",
    "Una función $f:\\mathbb{R} \\to \\mathbb{R}$ es unimodal si y solo sí tiene un solo mínimo global, denotado $x^*$. Así, sabemos que para $x \\leq x^*$, $f(x) $ es decreciente y para $x \\geq x^*$, $f(x)$ es creciente.\n",
    "\n",
    "Así, si encontramos un intervalo $[a,b]$ tal que existe $c \\in [a,b]$ con la propiedad de que $f(a) > f(c)$ y $f(c)< f(b)$, entonces eso nos garantiza que el mínimo de la función está en el intervalo  $[a,b]$, como muestra la siguiente figura\n",
    "\n",
    "![intervalo](https://lh3.googleusercontent.com/o5LOF85lvSzrqA2ZsDOqk-c4D0sPca47ZMBLlWE7_yLwLcpz0v7cHg1jP4qdJSiLLD4VvndbTHOoh1Y1a179GX298GPgoy_u2bGhiUsWu6I1k31QdeQWjOy6y4OpQY4wZwKeHeafDw=w300)\n",
    "\n",
    "### Método de la trisección\n",
    "\n",
    "Similar al método de la bisección para sacar raíces, podemos aprovechar este esquema para generar una sucesión anidada de intervalos en los que sabemos se encuentra el mínimo. Supongamos que encontramos el intervalo $[a,b]$ donde se cumple lo anterior. Ahora, nos tomamos los puntos $c_1, c_2$ que dividen el intervalo $[a,b]$ en tres subintervalos de igual longitud, como muestra la siguiente figura:\n",
    "\n",
    "![ref](https://lh3.googleusercontent.com/GzTcRu5cegI8NABgB1YNh1pmRbEqSftW2oETLcEu14u8W0NS0wfQUEXZkVjXP5FKbzths8xMXOb0hEYLU-ncQw_ldPtkFo-7OWnYVZNAZ9Q5bZQwz4NFgCI6X2pLF3Sw1rIlf6sRkw=w300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 5.1:\n",
    "\n",
    "Define una función `triseccion(a,b)` que te regrese los puntos $c_1$ y $c_2$ para el intervalo $[a,b]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así, gracias a que $f$ es unimodal, dependiendo de la relación de orden entre $f(c_1)$ y $f(c_2)$, podemos deducir cuál de los intervalos $[a,c_2]$ o $[c_1,b]$ contiene al mínimo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 5.2\n",
    "\n",
    "Deduce la regla para saber en cuál de los subintervalos $[a,c_2]$ o $[c_1,b]$ se encuentra el mínimo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al ya tener esta regla, procedemos de la misma forma del método de la bisección: ahora le aplicamos el procedimiento al nuevo intervalo encontrado y así sucesivamente. ¿Cuándo debemos de parar? hay dos opciones: podemos fijar un número de iteraciones o hacerlo hasta que la longitud del intervalo final sea muy chica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 6\n",
    "\n",
    "Define una función `miTriseccion(f,a,b,epsilon)` que implemente el método de la trisección hasta que la longitud del intervalo que contiene al mínimo sea menor que `epsilon`. Tu función debe de regresar los extremos del intervalo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 7\n",
    "\n",
    "Utiliza la función `miTriseccion(f,a,b,epsilon)` para encontrar el mínimo global de las siguientes funciones:\n",
    "\n",
    "1. $f_1(x) = x^2-4x - 5 $\n",
    "\n",
    "2. $f_2(x) = -e^{-(x+3)^2}$\n",
    "\n",
    "3. $f_3(x) = \\frac{-1}{(1+(x-3)^2)}$\n",
    "\n",
    "Resuelve analíticamente para encontrar la solución exacta y analiza el error absoluto como función de `epsilon`. Para eso, toma como solución dada por el algoritmo el punto medio del intervalo resultante.\n",
    "\n",
    "**Sugerencia**: primero grafica las funciones para que encuentren un intervalo $[a,b]$  que contenga al mínimo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caso general: funciones derivables\n",
    "\n",
    "La derivada nos indica hacia donde **crece** el valor de la función. Si yo quiero ir a un lugar donde la función es mínima, me debo de mover en **dirección contraria a la derivada**. Si la derivada es positiva, la función es creciente y debo de moverme hacia los negativos para encontrar menores valores de la función. Si la derivada es negativa, me debo de mover hacia los positivos para encontrar valores menores de la función.\n",
    "\n",
    "Podemos usar esta idea para proponer un esquema recursivo para ir construyendo una sucesión que se acerque a un mínimo.\n",
    "\n",
    "\n",
    "### Ejemplo 1: \n",
    "\n",
    "$$f(x) = x^2 - 5x + 6$$\n",
    "\n",
    "$$\n",
    "f'(x) = 2x-5\n",
    "$$\n",
    "\n",
    "$f'(x)>0$ indica que $f$ es creciente\n",
    "\n",
    "$f'(x)<0$ indica que $f$ es decreciente\n",
    "\n",
    "\n",
    "## Método de descenso de gradiente\n",
    "\n",
    "Supongamos que tenemos un valor $x_{k}$ tal que está cerca de un mínimo. ¿Cómo nos acercamos más a ese mínimo?\n",
    "\n",
    "$$\n",
    "x_{k+1} = x_{k} - \\alpha \\cdot f'(x_{k})\n",
    "$$\n",
    "\n",
    "con $0 < \\alpha < 1$. $\\alpha$ es el tamaño de paso. Para poder poder implementar la sucesión, tengo que dar un punto inicial $x_{inicial}$.\n",
    "\n",
    "$$\n",
    "x_{k} = \\begin{cases}\n",
    "    x_{inicial} & \\text{si } k=1 \\\\\n",
    "    x_{k-1} - \\alpha \\cdot f'(x_{k-1}) & \\text{si } k>1 \\\\\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 8\n",
    "\n",
    "Define una función `miDesGrad(f,x_inicial,alpha,n)` que implemente el método de descenso de gradiente para la función $f$, con punto inicial `x_inicial` y tamaño de paso `alpha`. Aproximen la derivada con una diferencia finita. La función debe regresar los primeros $n$ términos de la sucesión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 9\n",
    "\n",
    "Utiliza la función `miDesGrad` para encontrar los mínimos de las funciones del ejercicio 7.  Haz un análisis del error absoluto como función de $n$ y analiza la velocidad de convergencia. ¿Qué relación tiene dicha velocidad con `alpha` y `x_inicial`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 10\n",
    "\n",
    "Nuevamente, el método de descenso de gradiente es interesante por que se puede generalizar a funciones multivariadas $g:\\mathbb{R}^n \\to \\mathbb{R}$. ¿Cómo se te ocurre que se podría hacer dicha generalización? Muestra la fórmula de la sucesión para el caso multivariado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La visión general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método de Newton-Raphson es muy utilizado en la vida real para resolver ecuaciones trascendentales, pues es converge suficientemente rápido para muchas aplicaciones. Además, se puede aplicar a sistemas de ecuaciones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El área de optimización es fundamental para la computación. Todas las áreas importantes actualmente, en especial la estadística aplicada y el Machine Learning, en el fondo dependen de resolver problemas de optimización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método de la trisección no existe como tal, es una simplificación que yo me inventé de un método llamado **búsqueda de Fibonacci** para optimización. No les quise enseñar la búsqueda de Fibonacci por que es más complicada desde un punto de vista matemático."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método de descenso de gradiente es de los más utilizados para optimizar en todas las áreas que dependen de esto. En la práctica, hay muchos otros algoritmos que son refinaminamientos o variaciones de dicho algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
